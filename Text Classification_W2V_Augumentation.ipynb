{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbONgk2zT7lH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,precision_score,f1_score,recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM,GlobalMaxPooling1D,GRU,SpatialDropout1D\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "wordnet.synsets(\"subscribe\")\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"ThreadID\", \"rcontent\", \"Class\" ]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.7\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 100\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 100\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "# EXPORT\n",
        "\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n",
        "\n",
        "df=pd.read_excel('NYC2.xlsx')\n",
        "\n",
        "print(\"Dataset size:\", len(df))\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text, stem=True):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#AGUMENTATION####\n",
        "def find_synonyms(word):\n",
        "  synonyms = []\n",
        "  for synset in wordnet.synsets(word):\n",
        "    for syn in synset.lemma_names():\n",
        "      synonyms.append(syn)\n",
        "\n",
        "  # using this to drop duplicates while maintaining word order (closest synonyms comes first)\n",
        "  synonyms_without_duplicates = list(OrderedDict.fromkeys(synonyms))\n",
        "  return synonyms_without_duplicates\n",
        "\n",
        "def create_set_of_new_sentences(sentence, max_syn_per_word = 3):\n",
        "  new_sentences = []\n",
        "  for word in word_tokenize(sentence):\n",
        "    if len(word)<=3 : continue\n",
        "    for synonym in find_synonyms(word)[0:max_syn_per_word]:\n",
        "      synonym = synonym.replace('_', ' ') #restore space character\n",
        "      new_sentence = sentence.replace(word,synonym)\n",
        "      new_sentences.append(new_sentence)\n",
        "  return new_sentences\n",
        "\n",
        "def data_augment_synonym_replacement(data, column='subject'):\n",
        "  generated_data = pd.DataFrame([], columns=data.columns)\n",
        "  for index in data.index:\n",
        "    text_to_augment = data[column][index]\n",
        "    for generated_sentence in create_set_of_new_sentences(text_to_augment):\n",
        "      new_entry =  data.loc[[index]]\n",
        "      new_entry[column] = generated_sentence\n",
        "      generated_data=generated_data.append(new_entry)\n",
        "\n",
        "  generated_data_df = generated_data.drop_duplicates()\n",
        "  augmented_data= pd.concat([data.loc[:],generated_data_df], ignore_index=True)\n",
        "  return augmented_data\n",
        "#AGUMENTATION#####\n",
        "\n",
        "\n",
        "df.rcontent = df.rcontent.apply(lambda x: preprocess(x))\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=30)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))\n",
        "data_augment_synonym_replacement(df,column='rcontent')\n",
        "df_train1=df_train.append(data_augment_synonym_replacement(df,column='rcontent'))\n",
        "\n",
        "# Word2Vec\n",
        "documents = [_text.split() for _text in df_train1.rcontent]\n",
        "\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,\n",
        "                                            window=W2V_WINDOW,\n",
        "                                            min_count=W2V_MIN_COUNT,\n",
        "                                            workers=8)\n",
        "w2v_model.build_vocab(documents)\n",
        "\n",
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)\n",
        "\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "text=df['rcontent'].values\n",
        "#Tokenize Text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(df_train1.rcontent), maxlen=SEQUENCE_LENGTH)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(df_test.rcontent), maxlen=SEQUENCE_LENGTH)\n",
        "\n",
        "Y_train= pd.get_dummies(df_train1.Class).values\n",
        "Y_test= pd.get_dummies(df_test.Class).values\n",
        "\n",
        "\n",
        "\n",
        "print(\"x_train\", X_train.shape)\n",
        "print(\"y_train\", Y_train.shape)\n",
        "print()\n",
        "print(\"x_test\", X_test.shape)\n",
        "print(\"y_test\", Y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "#Embedding layer\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=X_train.shape[1], trainable=False)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=2),\n",
        "              EarlyStopping(monitor='val_accuracy',mode='max', min_delta=1, patience=2)]\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                     validation_split=0.2,\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])\n",
        "\n",
        "losst, accuracyt = model.evaluate(X_train, Y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\",accuracyt)\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
        "\n",
        "y_test_arg=np.argmax(Y_test,axis=1)\n",
        "precision = precision_score(y_test_arg,y_pred,average='macro')\n",
        "\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test_arg,y_pred,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test_arg,y_pred,average='macro')\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(embedding_layer)\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Conv1D(100,\n",
        "                 5,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "\n",
        "model1.add(GlobalMaxPooling1D())\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(400))\n",
        "model1.add(Dropout(0.25))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dense(3))\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model1.summary()\n",
        "\n",
        "history1=model1.fit(X_train, Y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "score1 = model1.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score1[1])\n",
        "print(\"LOSS:\",score1[0])\n",
        "\n",
        "\n",
        "y_pred1 = np.argmax(model1.predict(X_test),axis=1)\n",
        "\n",
        "y_test_arg1=np.argmax(Y_test,axis=1)\n",
        "precision = precision_score(y_test_arg1,y_pred1,average='macro')\n",
        "\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test_arg1,y_pred1,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test_arg1,y_pred1,average='macro')\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "#model2.add(Dropout(0.5))\n",
        "model2.add(GRU(100, dropout=0.25, recurrent_dropout=0.2))\n",
        "model2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks1 = [ ReduceLROnPlateau(monitor='val_loss', patience=3),\n",
        "              EarlyStopping(monitor='val_accuracy',mode='max', min_delta=1, patience=3)]\n",
        "\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "history2 = model2.fit(X_train, Y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1\n",
        "                    )\n",
        "\n",
        "score2 = model2.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score2[1])\n",
        "print(\"LOSS:\",score2[0])\n",
        "\n",
        "acc = history2.history['accuracy']\n",
        "val_acc = history2.history['val_accuracy']\n",
        "loss = history2.history['loss']\n",
        "val_loss = history2.history['val_loss']\n",
        "\n",
        "y_pred2 = np.argmax(model2.predict(X_test),axis=1)\n",
        "\n",
        "y_test_arg2=np.argmax(Y_test,axis=1)\n",
        "precision = precision_score(y_test_arg2,y_pred2,average='macro')\n",
        "\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test_arg2,y_pred2,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test_arg2,y_pred2,average='macro')\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}